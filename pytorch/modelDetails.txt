UNET(
  (ups): ModuleList(
    (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    (1): DoubleConv(
      (conv): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
    (3): DoubleConv(
      (conv): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
    (5): DoubleConv(
      (conv): Sequential(
        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (6): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
    (7): DoubleConv(
      (conv): Sequential(
        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (8): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))
    (9): DoubleConv(
      (conv): Sequential(
        (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
  )
  (downs): ModuleList(
    (0): DoubleConv(
      (conv): Sequential(
        (0): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (1): DoubleConv(
      (conv): Sequential(
        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (2): DoubleConv(
      (conv): Sequential(
        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (3): DoubleConv(
      (conv): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (4): DoubleConv(
      (conv): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
  )
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (bottleneck): DoubleConv(
    (conv): Sequential(
      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (final_conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))
  (sigmoid): Sigmoid()
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 32, 32]           1,440
       BatchNorm2d-2           [-1, 16, 32, 32]              32
              ReLU-3           [-1, 16, 32, 32]               0
            Conv2d-4           [-1, 16, 32, 32]           2,304
       BatchNorm2d-5           [-1, 16, 32, 32]              32
              ReLU-6           [-1, 16, 32, 32]               0
        DoubleConv-7           [-1, 16, 32, 32]               0
         MaxPool2d-8           [-1, 16, 16, 16]               0
            Conv2d-9           [-1, 32, 16, 16]           4,608
      BatchNorm2d-10           [-1, 32, 16, 16]              64
             ReLU-11           [-1, 32, 16, 16]               0
           Conv2d-12           [-1, 32, 16, 16]           9,216
      BatchNorm2d-13           [-1, 32, 16, 16]              64
             ReLU-14           [-1, 32, 16, 16]               0
       DoubleConv-15           [-1, 32, 16, 16]               0
        MaxPool2d-16             [-1, 32, 8, 8]               0
           Conv2d-17             [-1, 64, 8, 8]          18,432
      BatchNorm2d-18             [-1, 64, 8, 8]             128
             ReLU-19             [-1, 64, 8, 8]               0
           Conv2d-20             [-1, 64, 8, 8]          36,864
      BatchNorm2d-21             [-1, 64, 8, 8]             128
             ReLU-22             [-1, 64, 8, 8]               0
       DoubleConv-23             [-1, 64, 8, 8]               0
        MaxPool2d-24             [-1, 64, 4, 4]               0
           Conv2d-25            [-1, 128, 4, 4]          73,728
      BatchNorm2d-26            [-1, 128, 4, 4]             256
             ReLU-27            [-1, 128, 4, 4]               0
           Conv2d-28            [-1, 128, 4, 4]         147,456
      BatchNorm2d-29            [-1, 128, 4, 4]             256
             ReLU-30            [-1, 128, 4, 4]               0
       DoubleConv-31            [-1, 128, 4, 4]               0
        MaxPool2d-32            [-1, 128, 2, 2]               0
           Conv2d-33            [-1, 256, 2, 2]         294,912
      BatchNorm2d-34            [-1, 256, 2, 2]             512
             ReLU-35            [-1, 256, 2, 2]               0
           Conv2d-36            [-1, 256, 2, 2]         589,824
      BatchNorm2d-37            [-1, 256, 2, 2]             512
             ReLU-38            [-1, 256, 2, 2]               0
       DoubleConv-39            [-1, 256, 2, 2]               0
        MaxPool2d-40            [-1, 256, 1, 1]               0
           Conv2d-41            [-1, 512, 1, 1]       1,179,648
      BatchNorm2d-42            [-1, 512, 1, 1]           1,024
             ReLU-43            [-1, 512, 1, 1]               0
           Conv2d-44            [-1, 512, 1, 1]       2,359,296
      BatchNorm2d-45            [-1, 512, 1, 1]           1,024
             ReLU-46            [-1, 512, 1, 1]               0
       DoubleConv-47            [-1, 512, 1, 1]               0
  ConvTranspose2d-48            [-1, 256, 2, 2]         524,544
           Conv2d-49            [-1, 256, 2, 2]       1,179,648
      BatchNorm2d-50            [-1, 256, 2, 2]             512
             ReLU-51            [-1, 256, 2, 2]               0
           Conv2d-52            [-1, 256, 2, 2]         589,824
      BatchNorm2d-53            [-1, 256, 2, 2]             512
             ReLU-54            [-1, 256, 2, 2]               0
       DoubleConv-55            [-1, 256, 2, 2]               0
  ConvTranspose2d-56            [-1, 128, 4, 4]         131,200
           Conv2d-57            [-1, 128, 4, 4]         294,912
      BatchNorm2d-58            [-1, 128, 4, 4]             256
             ReLU-59            [-1, 128, 4, 4]               0
           Conv2d-60            [-1, 128, 4, 4]         147,456
      BatchNorm2d-61            [-1, 128, 4, 4]             256
             ReLU-62            [-1, 128, 4, 4]               0
       DoubleConv-63            [-1, 128, 4, 4]               0
  ConvTranspose2d-64             [-1, 64, 8, 8]          32,832
           Conv2d-65             [-1, 64, 8, 8]          73,728
      BatchNorm2d-66             [-1, 64, 8, 8]             128
             ReLU-67             [-1, 64, 8, 8]               0
           Conv2d-68             [-1, 64, 8, 8]          36,864
      BatchNorm2d-69             [-1, 64, 8, 8]             128
             ReLU-70             [-1, 64, 8, 8]               0
       DoubleConv-71             [-1, 64, 8, 8]               0
  ConvTranspose2d-72           [-1, 32, 16, 16]           8,224
           Conv2d-73           [-1, 32, 16, 16]          18,432
      BatchNorm2d-74           [-1, 32, 16, 16]              64
             ReLU-75           [-1, 32, 16, 16]               0
           Conv2d-76           [-1, 32, 16, 16]           9,216
      BatchNorm2d-77           [-1, 32, 16, 16]              64
             ReLU-78           [-1, 32, 16, 16]               0
       DoubleConv-79           [-1, 32, 16, 16]               0
  ConvTranspose2d-80           [-1, 16, 32, 32]           2,064
           Conv2d-81           [-1, 16, 32, 32]           4,608
      BatchNorm2d-82           [-1, 16, 32, 32]              32
             ReLU-83           [-1, 16, 32, 32]               0
           Conv2d-84           [-1, 16, 32, 32]           2,304
      BatchNorm2d-85           [-1, 16, 32, 32]              32
             ReLU-86           [-1, 16, 32, 32]               0
       DoubleConv-87           [-1, 16, 32, 32]               0
           Conv2d-88            [-1, 1, 32, 32]              17
================================================================
Total params: 7,779,617
Trainable params: 7,779,617
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.04
Forward/backward pass size (MB): 3.73
Params size (MB): 29.68
Estimated Total Size (MB): 33.44
----------------------------------------------------------------
None
